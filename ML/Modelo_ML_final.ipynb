{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fransindi/recomendacion_amazon/blob/master/Modelo_ML_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STo428gs1_ZA"
      },
      "source": [
        "# Proceso ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thW8XESN2E0Z"
      },
      "source": [
        "### Instalamos las librerias necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ExwHeOyurAh",
        "outputId": "72e9ac3f-084f-4c17-b5ac-5fc07b3e05a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.3)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3163341 sha256=fb75652578ceab1879a191503e9602594813414da7457fb0ce6aa3c9755e2240\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.3\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvWj9VHP2I-h"
      },
      "source": [
        "### importamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1ykCgKBwAUl",
        "outputId": "a9f62aab-9568-4fc0-e911-776385f9d847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKs2Twfq2K2s"
      },
      "source": [
        "### Importamos las librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUhex17ImK_G",
        "outputId": "ac554356-52a5-43c4-8bf2-6d5502df7723"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Importar librerías necesarias\n",
        "import csv\n",
        "import pandas as pd\n",
        "import json\n",
        "import gzip\n",
        "import ast\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from surprise import Dataset, Reader, SVD, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssG0Mxxh2Nf0"
      },
      "source": [
        "### creamos funcion para separar y extraer las palabras relevantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63DXVfs1u5S7"
      },
      "outputs": [],
      "source": [
        "def extract_keywords(text):\n",
        "    # Tokenizar el texto en palabras\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Obtener las stopwords en español\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Filtrar las palabras clave quitando las stopwords y las que tienen menos de 3 caracteres\n",
        "    keywords = [word.lower() for word in words if word.lower() not in stop_words and len(word) > 2]\n",
        "\n",
        "    return keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQqPn-zU2SSN"
      },
      "source": [
        "### creamos listas con los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoIJsTCNszGl"
      },
      "outputs": [],
      "source": [
        "archivos_review = ['/content/drive/MyDrive/Colab Notebooks/Amazon sample clean datasets/All_Beauty_Processed.json', '/content/drive/MyDrive/Colab Notebooks/Amazon sample clean datasets/Digital_Music_Processed.json','/content/drive/MyDrive/Colab Notebooks/Amazon sample clean datasets/Pet_Supplies_Processed.json','/content/drive/MyDrive/Colab Notebooks/Amazon sample clean datasets/Toys_and_Games_Processed.json']\n",
        "archivos_meta = ['/content/drive/MyDrive/Colab Notebooks/Amazon sample clean datasets/All_Beauty_meta_Processed.json', '/content/drive/MyDrive/Colab Notebooks/Amazon sample clean datasets/Digital_Music_meta_Processed.json','/content/drive/MyDrive/Colab Notebooks/Amazon sample clean datasets/Pet_Supplies_meta_Processed.json','/content/drive/MyDrive/Colab Notebooks/Amazon sample clean datasets/Toys_and_Games_meta_Processed.json']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEHShQVJ2Ur-"
      },
      "source": [
        "### creamos la funcion 'proceso_ml' para que ingresando los datos automaticamente haga el proceso de separacion de palabras con NLTK y usamos SVD para entrenar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZxq3WfnuEqe"
      },
      "outputs": [],
      "source": [
        "def proceso_ml(json_review, json_meta):\n",
        "    with open(json_review, 'r') as archivo_json:\n",
        "    # Lee líneas del archivo y carga cada línea como un objeto JSON\n",
        "        objetos_json = [json.loads(line) for line in archivo_json]\n",
        "\n",
        "    # Ahora, objetos_json es una lista de diccionarios\n",
        "    # Creamos Dataframe de reviews\n",
        "    df = pd.DataFrame(objetos_json)\n",
        "\n",
        "    # Abre el archivo JSON\n",
        "    with open(json_meta, 'r') as archivo_json:\n",
        "    # Lee líneas del archivo y carga cada línea como un objeto JSON\n",
        "        objetos_json = [json.loads(line) for line in archivo_json]\n",
        "\n",
        "    # Ahora, objetos_json es una lista de diccionarios\n",
        "    dfmeta = pd.DataFrame(objetos_json)\n",
        "\n",
        "    df_completo = df.merge(dfmeta, on=\"CodigoProducto\")\n",
        "\n",
        "    # Crear un objeto Reader para cargar los datos\n",
        "    reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "    # Cargar los datos desde el DataFrame completo\n",
        "    data = Dataset.load_from_df(df_completo[[\"CodigoUsuario\", \"CodigoProducto\", \"Overall\"]], reader)\n",
        "\n",
        "    # Dividir los datos en conjuntos de entrenamiento y prueba (80% para entrenamiento, 20% para prueba)\n",
        "    trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "    # Elegir un algoritmo de recomendación (en este caso, SVD)\n",
        "    algo = SVD()\n",
        "\n",
        "    # Entrenar el modelo en el conjunto de entrenamiento\n",
        "    algo.fit(trainset)\n",
        "\n",
        "    # Obtener las predicciones de calificaciones para el conjunto de prueba\n",
        "    predictions = algo.test(testset)\n",
        "\n",
        "    # Evaluar el rendimiento del modelo calculando el RMSE\n",
        "    rmse = accuracy.rmse(predictions)\n",
        "\n",
        "    product_keywords = dfmeta['NombreProducto'].apply(extract_keywords)\n",
        "    all_keywords = [keyword for keywords in product_keywords for keyword in keywords]\n",
        "    keyword_counts = Counter(all_keywords)\n",
        "\n",
        "    top_keywords = [keyword for keyword, count in keyword_counts.most_common(1000)]\n",
        "\n",
        "        # Define tus 10 palabras clave\n",
        "    palabras_clave = top_keywords\n",
        "\n",
        "    # Crea un DataFrame para almacenar las recomendaciones\n",
        "    recomendaciones_df = pd.DataFrame(columns=[\"PalabraClave\", \"Recomendaciones\"])\n",
        "\n",
        "    # Crear un objeto Reader para cargar los datos\n",
        "    reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "    # Cargar los datos desde el DataFrame completo\n",
        "    data = Dataset.load_from_df(df_completo[[\"CodigoUsuario\", \"CodigoProducto\", \"Overall\"]], reader)\n",
        "\n",
        "    # Elegir un algoritmo de recomendación (en este caso, SVD)\n",
        "    algo = SVD()\n",
        "\n",
        "    # Entrenar el modelo en el conjunto de datos completo\n",
        "    trainset = data.build_full_trainset()\n",
        "    algo.fit(trainset)\n",
        "\n",
        "    # Crear un diccionario para almacenar las recomendaciones por palabra clave\n",
        "    recomendaciones_por_palabra = {}\n",
        "\n",
        "    # Iterar a través de las palabras clave\n",
        "    for palabra_clave in palabras_clave:\n",
        "        recomendaciones = []\n",
        "\n",
        "        # Crear una expresión regular para buscar la palabra clave en el nombre o descripción del producto\n",
        "        expresion_regular = re.compile(f\".*{re.escape(palabra_clave)}.*\", flags=re.IGNORECASE)\n",
        "\n",
        "        # Buscar productos que coincidan con la palabra clave\n",
        "        productos_coincidentes = dfmeta[dfmeta['NombreProducto'].apply(lambda x: bool(expresion_regular.match(x)))]\n",
        "\n",
        "        for codigo_producto in productos_coincidentes['CodigoProducto']:\n",
        "            if codigo_producto not in df_completo['CodigoProducto'].values:\n",
        "                recomendaciones.append(codigo_producto)\n",
        "\n",
        "        # Obtener los nombres y precios de los productos correspondientes a los códigos\n",
        "        productos_recomendados = dfmeta[dfmeta['CodigoProducto'].isin(recomendaciones)][['NombreProducto', 'PrecioProducto']]\n",
        "        recomendaciones_por_palabra[palabra_clave] = productos_recomendados[:5].to_dict(orient='records')\n",
        "\n",
        "    # Agregar las recomendaciones al DataFrame\n",
        "    for palabra_clave, recomendaciones in recomendaciones_por_palabra.items():\n",
        "        recomendaciones_df = recomendaciones_df.append({\"PalabraClave\": palabra_clave, \"Recomendaciones\": recomendaciones}, ignore_index=True)\n",
        "\n",
        "    # Guardar las recomendaciones en formato JSON\n",
        "    recomendaciones_df.to_json(\"recomendaciones.json\", orient=\"records\")\n",
        "\n",
        "    return recomendaciones_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rCJ1To82lXw"
      },
      "source": [
        "### creamos los dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftveuh27vZk_",
        "outputId": "94dbba19-7466-40c3-eaa4-ad0219f7b25d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 1.3675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
          ]
        }
      ],
      "source": [
        "df_all_beauty = proceso_ml(archivos_review[0], archivos_meta[0])\n",
        "df_digital_music = proceso_ml(archivos_review[1], archivos_meta[1])\n",
        "df_pets = proceso_ml(archivos_review[2], archivos_meta[2])\n",
        "df_toys = proceso_ml(archivos_review[3], archivos_meta[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2TOqcg82oDk"
      },
      "source": [
        "### Extraemos las tablas creadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzdyKWnP1vQi"
      },
      "outputs": [],
      "source": [
        "df_all_beauty.to_csv('ml_all_beauty.csv')\n",
        "df_digital_music.to_csv('ml_digital_music.csv')\n",
        "df_pets.to_csv('ml_pet.csv')\n",
        "df_toys.to_csv('ml_toys.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNP7AR8/tY0LSpXg8HEX9Bg",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
